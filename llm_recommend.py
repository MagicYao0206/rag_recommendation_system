import os
import sys
import pandas as pd
from retrieval import retrieve_products  

# å…¨å±€ç¦ç”¨å¤–ç½‘è¯·æ±‚ï¼ˆä¿æŒç¦»çº¿ï¼‰
os.environ["HF_HUB_OFFLINE"] = "1"
os.environ["TRANSFORMERS_OFFLINE"] = "1"

def init_llama_model():
    """åˆå§‹åŒ–Llama-2æ¨¡å‹ï¼ˆä½¿ç”¨llama.cppï¼‰"""
    try:
        from llama_cpp import Llama
        
        # æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆè¯·æ›¿æ¢ä¸ºä½ çš„å®é™…è·¯å¾„ï¼‰
        model_path = r"F:\rag_recommendation_system\models\llama-2-7b-chat.Q4_K_M.gguf"
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼š{model_path}")

        # åˆå§‹åŒ–Llamaæ¨¡å‹ï¼ˆé…ç½®å‚æ•°æ ¹æ®ç¡¬ä»¶è°ƒæ•´ï¼‰
        llm = Llama(
            model_path=model_path,
            n_ctx=2048,          # ä¸Šä¸‹æ–‡çª—å£å¤§å°
            n_threads=4,         # çº¿ç¨‹æ•°ï¼ˆæ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ï¼‰
            n_gpu_layers=20,     # GPUåŠ é€Ÿå±‚æ•°ï¼ˆ0è¡¨ç¤ºä»…ç”¨CPUï¼‰
            verbose=False        # å…³é—­è¯¦ç»†æ—¥å¿—
        )
        
        print("âœ… Llamaæ¨¡å‹åŠ è½½æˆåŠŸ")
        return llm
    
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
        return None

def generate_recommendation_stream(retrieved_data, llm):
    """åŸºäºLlamaæ¨¡å‹æµå¼ç”Ÿæˆæ¨èç»“æœ"""
    # æ„é€ å•†å“æ£€ç´¢æ–‡æœ¬ï¼ˆä¿æŒä¸å˜ï¼‰
    product_text = "\n".join([
        f"{i+1}. {row['title']}ï¼ˆç›¸ä¼¼åº¦{row['similarity_score']:.2f}ï¼‰"
        for i, row in retrieved_data.iterrows()
    ])
    
    # æç¤ºè¯æ¨¡æ¿ï¼ˆä¿æŒä¸å˜ï¼‰
    prompt = f"""[INST] <<SYS>>
    ä½ æ˜¯ä¸“ä¸šçš„ç¾å¦†å•†å“æ¨èå¸ˆï¼ŒåŸºäºæä¾›çš„æ£€ç´¢ç»“æœç”Ÿæˆç®€æ´å‡†ç¡®çš„æ¨èç†ç”±ã€‚
    <</SYS>>

    åŸºäºä»¥ä¸‹æ£€ç´¢ç»“æœç”Ÿæˆ3æ¡æ¨èç†ç”±ï¼Œæ¯æ¡ä¸è¶…è¿‡50å­—ï¼Œå¸¦ç¼–å·ä¸”å•ç‹¬å ä¸€è¡Œï¼Œä»…è¾“å‡ºæ¨èå†…å®¹ï¼š
    {product_text} [/INST]"""
    
    # æµå¼ç”Ÿæˆé…ç½®
    stream = llm(
        prompt=prompt,
        max_tokens=300,
        temperature=0.7,
        stop=["</s>"],
        echo=False,
        stream=True  # å¯ç”¨æµå¼è¾“å‡º
    )
    
    # é€ token è¾“å‡ºç»“æœ
    response = []
    for chunk in stream:
        token = chunk["choices"][0]["text"]
        response.append(token)
        print(token, end="", flush=True)  # å®æ—¶åˆ·æ–°è¾“å‡º
    print()  # ç”Ÿæˆç»“æŸåæ¢è¡Œ
    return "".join(response)

# æµ‹è¯•ä¸»æµç¨‹
if __name__ == "__main__":
    query = "é€‚åˆæ•æ„Ÿè‚Œçš„é¢éœœ"
    retrieved_products = retrieve_products(query, top_k=3)
    llm = init_llama_model()
    if llm:
        print("\nğŸ¯ å•†å“æ¨èç†ç”±ï¼š")
        print(generate_recommendation_stream(retrieved_products, llm))
        llm.close()  # æ˜¾å¼å…³é—­æ¨¡å‹